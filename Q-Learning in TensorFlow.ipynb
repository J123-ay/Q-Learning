{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-Learning in TensorFlow.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "dyH7dibmy7yz",
        "colab_type": "code",
        "outputId": "afd86914-de84-43e1-facf-1b2cfbb31619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/22/4ff09745ade385ffe707fb5f053548f0f6a6e7d5e98a2b9d6c07f5b931a7/gym-0.10.9.tar.gz (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 16.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Collecting pyglet>=1.2.0 (from gym)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 18.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.10.15)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Running setup.py bdist_wheel for gym ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/6c/3a/0e/b86dee98876bb56cdb482cc1f72201035e46d1baf69d10d028\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, gym\n",
            "Successfully installed gym-0.10.9 pyglet-1.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AVv65MkKzOmG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2rz7AEvpzVsn",
        "colab_type": "code",
        "outputId": "a9a013be-94c0-4849-eeca-4ff20793735f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# Create the env\n",
        "env_name = 'FrozenLake-v0'\n",
        "env = gym.make(env_name)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "0I9e7YdgzbwL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aab674f0-029e-44f1-c0ce-a5b57a887d8e"
      },
      "cell_type": "code",
      "source": [
        "# Get the action_space and observation_space\n",
        "action_size = env.action_space.n\n",
        "observation_size = env.observation_space.n\n",
        "\n",
        "print ('Action Space Size: {}'.format(action_size),\n",
        "        'Observation Space Size: {}'.format(observation_size))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action Space Size: 4 Observation Space Size: 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gjEnavj7zvIt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Rjo4Y9Lz0st",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The Q-Network"
      ]
    },
    {
      "metadata": {
        "id": "zT_jeOvb55Fw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameters of the network\n",
        "lr = 0.1\n",
        "max_steps = 99\n",
        "gamma = .95\n",
        "total_train_episodes = 10000\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = .3\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.01\n",
        "decay_rate = 0.01\n",
        "\n",
        "# Other paramters\n",
        "interval = 500\n",
        "tboard_dir = './tboard/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d4fJakL-0YK-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Declare the placeholders\n",
        "inputs = tf.placeholder(shape=[1, observation_size], dtype=tf.float32)\n",
        "tqvals = tf.placeholder(shape=[1, action_size], dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ml9W8yOm2J_x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "\n",
        "W1 = tf.get_variable('qweights', shape=[observation_size, action_size], \n",
        "                     initializer=tf.random_normal_initializer())\n",
        "\n",
        "pout = tf.matmul(inputs, W1)\n",
        "predict = tf.argmax(pout, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gvuee232SH6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = tf.reduce_sum(tf.square(tqvals - pout))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
        "train_op = optimizer.minimize(criterion)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vdbMhKCoXllH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TensorBoard Ops\n",
        "def attach_summaries(var):\n",
        "    mean = tf.reduce_mean(var)\n",
        "    tf.summary.scalar('mean', mean)\n",
        "    \n",
        "    stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "    tf.summary.scalar('stddev', stddev)\n",
        "    \n",
        "    tf.summary.scalar('max', tf.reduce_max(var))\n",
        "    tf.summary.scalar('min', tf.reduce_min(var))\n",
        "    tf.summary.histogram('histogram', var)\n",
        "    \n",
        "attach_summaries(W1)\n",
        "attach_summaries(criterion)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FmKwgDuQ5hZZ",
        "colab_type": "code",
        "outputId": "45696abd-4d58-4357-c2c0-b2dd483688ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2518
        }
      },
      "cell_type": "code",
      "source": [
        "## Traning the network\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "reward_list = []\n",
        "steps_list = []\n",
        "\n",
        "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
        "    \n",
        "    merged = tf.summary.merge_all()\n",
        "    train_writer = tf.summary.FileWriter(tboard_dir + env_name + '/train/',\n",
        "                                         sess.graph)\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for episode in range(total_train_episodes):\n",
        "        \n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_rewards = 0\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            \n",
        "            action, curr_qvals = sess.run([predict, pout], \n",
        "                                       feed_dict={inputs: np.identity(observation_size)[state:state+1]})\n",
        "            action = action[0]\n",
        "            \n",
        "            exp_prob = np.random.uniform()\n",
        "            if exp_prob < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "                \n",
        "                \n",
        "            #print (action, curr_qvals)\n",
        "            \n",
        "            # Step with the chosen action\n",
        "            new_state, rewards, done, info = env.step(action)\n",
        "            \n",
        "            # Get the qvalues for the new state\n",
        "            next_qvals = sess.run([pout], \n",
        "                         feed_dict={inputs: np.identity(observation_size)[new_state:new_state+1]})\n",
        "            \n",
        "            # Get the target output for current state\n",
        "            max_next_qvals = np.max(next_qvals)\n",
        "            target_q = curr_qvals\n",
        "            target_q[0, action] = rewards + (gamma * max_next_qvals)\n",
        "            \n",
        "            # Train the network using the target Q values\n",
        "            _ = sess.run([train_op], feed_dict={inputs: np.identity(observation_size)[state:state+1],\n",
        "                                                tqvals: target_q})\n",
        "            \n",
        "            \n",
        "            total_rewards += rewards\n",
        "            \n",
        "            if done or step == max_steps:\n",
        "                if episode % interval == 0:\n",
        "                    env.render()\n",
        "                    print ('Maximum steps: {}'.format(step))\n",
        "                break\n",
        "                \n",
        "            state = new_state\n",
        "        \n",
        "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
        "        \n",
        "        reward_list.append(total_rewards)\n",
        "        steps_list.append(step)\n",
        "        \n",
        "        if episode % interval == 0:\n",
        "            print ('Episode {}/{}'.format(episode, total_train_episodes),\n",
        "                   'Reward: {}'.format(total_rewards))\n",
        "        \n",
        "print ('Agent Trained successfully!!')\n",
        "print ('Successful Episodes: {}/{}'.format(sum(reward_list), total_train_episodes))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 2\n",
            "Episode 0/10000 Reward: 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
            "  return umr_maximum(a, axis, None, out, keepdims)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 15\n",
            "Episode 500/10000 Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 17\n",
            "Episode 1000/10000 Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 4\n",
            "Episode 1500/10000 Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 33\n",
            "Episode 2000/10000 Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 25\n",
            "Episode 2500/10000 Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 4\n",
            "Episode 3000/10000 Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 11\n",
            "Episode 3500/10000 Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 5\n",
            "Episode 4000/10000 Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 5\n",
            "Episode 4500/10000 Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 14\n",
            "Episode 5000/10000 Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 11\n",
            "Episode 5500/10000 Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 2\n",
            "Episode 6000/10000 Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 3\n",
            "Episode 6500/10000 Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 28\n",
            "Episode 7000/10000 Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 8\n",
            "Episode 7500/10000 Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 46\n",
            "Episode 8000/10000 Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 37\n",
            "Episode 8500/10000 Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 5\n",
            "Episode 9000/10000 Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "Maximum steps: 32\n",
            "Episode 9500/10000 Reward: 0.0\n",
            "Agent Trained successfully!!\n",
            "Successful Episodes: 0.0/10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bT76N2H5TUv0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Testing Hyperparameters\n",
        "total_test_episodes = 100\n",
        "max_steps = 99\n",
        "\n",
        "interval = 1\n",
        "show_each_episode = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SZ4UFFoU9u2q",
        "colab_type": "code",
        "outputId": "dc8a7be9-f3cb-4b7f-8b75-04071f366389",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Testing out the network\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "test_reward_list = []\n",
        "\n",
        "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
        "    \n",
        "    merged = tf.summary.merge_all()\n",
        "    test_writer = tf.summary.FileWriter(tboard_dir + env_name + '/test/',\n",
        "                                        sess.graph)\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for episode in range(total_test_episodes):\n",
        "        \n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        final_reward = 0\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            next_action = sess.run([predict], \n",
        "                          feed_dict={inputs: np.identity(observation_size)[state:state+1]})\n",
        "            \n",
        "            next_action = next_action[0][0]\n",
        "            \n",
        "            next_state, rewards, done, info = env.step(next_action)\n",
        "            \n",
        "            final_reward += rewards\n",
        "            \n",
        "            if done or step == max_steps:\n",
        "                if episode % interval == 0 and show_each_episode:\n",
        "                    env.render()\n",
        "                    print ('Episode {}/{}'.format(episode+1, total_episodes),\n",
        "                           'Rewards: {}'.format(all_rewards),\n",
        "                           'Maximum steps: {}'.format(step+1))\n",
        "                break\n",
        "                \n",
        "        test_reward_list.append(final_reward)\n",
        "                \n",
        "    print ('Successful Episodes: {}/{}'.format(sum(test_reward_list), total_test_episodes))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successful Episodes: 3.0/100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mVWGRrr4-tb_",
        "colab_type": "code",
        "outputId": "146851c8-eedd-4521-bf95-8b5860cecc5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "cell_type": "code",
      "source": [
        "# TensorBoard Setup\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('tensorboard --logdir=./tboard/FrozenLake-v0/ &')\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-30 08:37:55--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.45.111.123, 54.165.51.142, 54.174.228.92, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.45.111.123|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5363700 (5.1M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]   5.11M  9.77MB/s    in 0.5s    \n",
            "\n",
            "2018-11-30 08:37:56 (9.77 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [5363700/5363700]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: ngrok                   \n",
            "http://f794266a.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}